library(ggrepel)
library(ggfortify)
library(ggdendro)
library(goophi)
set.seed(1234)
cleaned_data <- read.csv("~/git/goophi_dev/data/winequality-red.csv", sep = ",")
cleaned_data
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "quality"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "quality ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "rpart"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::decisionTree_phi(engine = engine,
mode = mode)
model
#### (4) Grid serach CV ####
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
tree_depth(range = c(10, 30)),
min_n(range = c(2, 10)),
cost_complexity = sample_prop(c(0.1, 0.5)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
grid_search_result
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "rmse",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
last_fitted_model %>% collect_metrics()
ames_test_res <- last_fitted_model$.predictions[[1]][1]
ames_test_res <- bind_cols(ames_test_res, data_test %>% select(quality))
ames_test_res
# regression plot
last_fitted_model %>% collect_metrics()
last_fitted_model %>%
tune::collect_predictions() %>%
ggplot(aes(x = quality, y = last_fitted_model$.predictions[[1]]$.pred)) + geom_abline(color = "gray50", lty = 2) + geom_point(alpha = 0.5) + coord_obs_pred()
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = quality, estimate = .pred)
source("~/Git/goophi_dev/etc/dtModel.R")
?parsnip::nearest_neighbor
library(tidyverse)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(ggrepel)
library(ggfortify)
library(ggdendro)
library(goophi)
set.seed(1234)
cleaned_data <- read.csv("~/git/goophi_dev/data/winequality-red.csv", sep = ",")
cleaned_data
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "quality"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "quality ~ ."
pcaThres <- "0.7"
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "kknn"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::knn_phi(engine = engine,
mode = mode)
model
parameterGrid <- dials::grid_regular(
neighbors(range = c(5, 10)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
grid_search_result
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "rmse",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
#### regression ####
last_fitted_model %>% collect_metrics()
ames_test_res <- last_fitted_model$.predictions[[1]][1]
ames_test_res <- bind_cols(ames_test_res, data_test %>% select(quality))
ames_test_res
# regression plot
last_fitted_model %>% collect_metrics()
last_fitted_model %>%
tune::collect_predictions() %>%
ggplot(aes(x = quality, y = last_fitted_model$.predictions[[1]]$.pred)) + geom_abline(color = "gray50", lty = 2) + geom_point(alpha = 0.5) + coord_obs_pred()
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = quality, estimate = .pred)
source("~/Git/goophi_dev/etc/knnModel.R")
parsnip::boost_tree?
?parsnip::boost_tree
?parsnip::boost_tree
library(tidyverse)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(ggrepel)
library(ggfortify)
library(ggdendro)
library(goophi)
library(treesnip)
cleaned_data <- read.csv("~/git/goophi_dev/data/winequality-red.csv", sep = ",")
cleaned_data
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "quality"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "quality ~ ."
pcaThres <- "0.7"
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "lightgbm"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
mtry(range = c(1, 10)),
trees(range = c(100, 500)),
min_n(range = c(20, 60)),
tree_depth(range = c(0, 10)),
learn_rate = sample_prop(c(0.1, 0.5)),
loss_reduction(range = c(1, 10)),
#sample_size = sample_prop(c(0.5, 1)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
grid_search_result
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "rmse",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
#### regression ####
last_fitted_model %>% collect_metrics()
ames_test_res <- last_fitted_model$.predictions[[1]][1]
ames_test_res <- bind_cols(ames_test_res, data_test %>% select(quality))
ames_test_res
# regression plot
last_fitted_model %>% collect_metrics()
last_fitted_model %>%
tune::collect_predictions() %>%
ggplot(aes(x = quality, y = last_fitted_model$.predictions[[1]]$.pred)) + geom_abline(color = "gray50", lty = 2) + geom_point(alpha = 0.5) + coord_obs_pred()
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = quality, estimate = .pred)
?parsnip::logistic_reg
library(tidyverse)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(ggrepel)
library(ggfortify)
library(ggdendro)
library(goophi)
set.seed(1234)
cleaned_data <- read.csv("~/git/goophi_dev/data/winequality-red.csv", sep = ",")
cleaned_data
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "quality"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "quality ~ ."
pcaThres <- "0.7"
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "glm"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::logisticRegression_phi(engine = engine,
mode = mode)
model
#### (4) Grid serach CV ####
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
penalty = sample_prop(c(0.1, 0.5)),
mixture(range = c(2, 10)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
####################
parameterGrid
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "rmse",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
library(tidyverse)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(ggrepel)
library(ggfortify)
library(ggdendro)
library(goophi)
set.seed(1234)
cleaned_data <- read.csv("~/git/goophi_dev/data/winequality-red.csv", sep = ",")
cleaned_data
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "quality"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "quality ~ ."
pcaThres <- "0.7"
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "glm"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::logisticRegression_phi(engine = engine,
mode = mode)
model
#### (4) Grid serach CV ####
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
penalty = sample_prop(c(0.1, 0.5)),
mixture(range = c(2, 10)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
#### regression ####
# engine, mode 사용자로부터 입력 받습니다
engine = "glm"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::logisticRegression_phi(engine = engine,
mode = mode)
model
?parsnip::linear_reg
if (! ("devtools" %in% rownames(installed.packages()))) { install.packages("devtools") }
base::require("devtools")
if (! ("roxygen2" %in% rownames(installed.packages()))) { install.packages("roxygen2") }
base::require("roxygen2")
if (! ("testthat" %in% rownames(installed.packages()))) { install.packages("testthat") }
base::require("testthat")
if (! ("knitr" %in% rownames(installed.packages()))) { install.packages("knitr") }
base::require("knitr")
devtools::document()
devtools::document()
devtools::document()
library(goophi)
library(tidyverse)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(ggrepel)
library(ggfortify)
library(ggdendro)
library(goophi)
set.seed(1234)
cleaned_data <- read.csv("~/git/goophi_dev/data/winequality-red.csv", sep = ",")
cleaned_data
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "quality"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "quality ~ ."
pcaThres <- "0.7"
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### regression ####
# engine, mode 사용자로부터 입력 받습니다
engine = "glm"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::logisticRegression_phi(engine = engine,
mode = mode)
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
penalty = sample_prop(c(0.1, 0.5)),
mixture(range = c(2, 10)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "rmse",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
#### regression ####
last_fitted_model %>% collect_metrics()
ames_test_res <- last_fitted_model$.predictions[[1]][1]
ames_test_res <- bind_cols(ames_test_res, data_test %>% select(quality))
ames_test_res
# regression plot
last_fitted_model %>% collect_metrics()
last_fitted_model %>%
tune::collect_predictions() %>%
ggplot(aes(x = quality, y = last_fitted_model$.predictions[[1]]$.pred)) + geom_abline(color = "gray50", lty = 2) + geom_point(alpha = 0.5) + coord_obs_pred()
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = quality, estimate = .pred)
#### regression ####
# engine, mode 사용자로부터 입력 받습니다
engine = "glm"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::logisticRegression_phi(engine = engine,
mode = mode)
model
#### regression ####
# engine, mode 사용자로부터 입력 받습니다
engine = "lm"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::logisticRegression_phi(engine = engine,
mode = mode)
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
penalty = sample_prop(c(0.1, 0.5)),
mixture(range = c(2, 10)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
penalty = sample_prop(c(0.1, 0.5)),
mixture(range = c(0, 1)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "rmse",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
#### regression ####
last_fitted_model %>% collect_metrics()
ames_test_res <- last_fitted_model$.predictions[[1]][1]
ames_test_res <- bind_cols(ames_test_res, data_test %>% select(quality))
ames_test_res
# regression plot
last_fitted_model %>% collect_metrics()
last_fitted_model %>%
tune::collect_predictions() %>%
ggplot(aes(x = quality, y = last_fitted_model$.predictions[[1]]$.pred)) + geom_abline(color = "gray50", lty = 2) + geom_point(alpha = 0.5) + coord_obs_pred()
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = quality, estimate = .pred)
