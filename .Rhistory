)
<<<<<<< HEAD
=======
# engine, mode 사용자로부터 입력 받습니다
engine = "klaR"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::naiveBayes_phi(engine = engine,
mode = mode)
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
smoothness(range = c(0.5, 1.5)),
Laplace(range = c(0, 3))
)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
# parameter grid를 적용한 cross validation을 수행합니다
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
rec
data_train
parameterGrid
# parameter grid를 적용한 cross validation을 수행합니다
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
#######################
tune_nb <- workflows::workflow() %>%
workflows::add_recipe(rec) %>%
workflows::add_model(model)
## v-fold cv
folds <- rsample::vfold_cv(data_train, v = 2)
## grid search CV
regular_res <- tune_grid(tune_nb, resamples = folds, grid = parameter_grid) # warnings
## grid search CV
regular_res <- tune_grid(tune_nb, resamples = folds, grid = parameterGrid) # warnings
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune")
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "klaR"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::naiveBayes_phi(engine = engine,
mode = mode)
model
#### (4) Grid serach CV ####
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
smoothness(range = c(0.5, 1.5)),
Laplace(range = c(0, 3))
)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
# parameter grid를 적용한 cross validation을 수행합니다
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
grid_search_result
tune_nb <- workflows::workflow() %>%
workflows::add_recipe(rec) %>%
workflows::add_model(model)
## v-fold cv
folds <- rsample::vfold_cv(data_train, v = 2)
## grid search CV
regular_res <- tune_grid(tune_nb, resamples = folds, grid = parameterGrid) # warnings
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune", dependencies=TRUE)
devtools::install_github("tidymodels/tune", dependencies=TRUE)
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune")
usethis::create_github_token()
usethis::edit_r_environ()
devtools::install_github("tidymodels/tune")
usethis::edit_r_environ()
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune")
devtools::install_github("tidymodels/tune")
>>>>>>> 8906c592ae2fe4dc324f2b903cdd54cb904a5190
?parsnip::boost_tree
library(xgboost)
?xgboost::boost_tree
?xgboost::boost_tree
xgboost::boost_tree
# 함수 추가 시 roxygen 주석을 포함시켜 작성하고, 아래 코드로 주석을 .Rd 파일로 전환 및 NAMESPACE에 추가
devtools::document()
library(goophi)
# 함수 추가 시 roxygen 주석을 포함시켜 작성하고, 아래 코드로 주석을 .Rd 파일로 전환 및 NAMESPACE에 추가
devtools::document()
devtools::document()
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
library(lightgbm)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
dplyr::select(-c(PassengerId, Name, Cabin, Ticket)) %>%
dplyr::mutate(across(where(is.character), factor)) %>%
dplyr::mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
model
library(xgboost)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
library(xgboost)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
dplyr::select(-c(PassengerId, Name, Cabin, Ticket)) %>%
dplyr::mutate(across(where(is.character), factor)) %>%
dplyr::mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
devtools::document()
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
remotes::install_github("curso-r/treesnip")
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
dplyr::select(-c(PassengerId, Name, Cabin, Ticket)) %>%
dplyr::mutate(across(where(is.character), factor)) %>%
dplyr::mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
library(goophi)
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
library(treesnip)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
library(treesnip)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
dplyr::select(-c(PassengerId, Name, Cabin, Ticket)) %>%
dplyr::mutate(across(where(is.character), factor)) %>%
dplyr::mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
model
?parsnip::boost_tree
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
mtry(range = c(1, 5)),
trees(range = c(100, 500)),
min_n(range = c(20, 50)),
tree_depth(range = c(-1, 10)),
learn_rate(range = c(0.1, 1)),
loss_reduction(range = c(0, 10)),
sample_size(range = c(0.01, 1)),
levels = 5)
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
mtry(range = c(1, 5)),
trees(range = c(100, 500)),
min_n(range = c(20, 50)),
tree_depth(range = c(-1, 10)),
learn_rate(range = c(0.1, 1)),
loss_reduction(range = c(0, 10)),
#sample_size(range = c(0.01, 1)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
library(goophi)
devtools::document()
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
library(treesnip)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
dplyr::select(-c(PassengerId, Name, Cabin, Ticket)) %>%
dplyr::mutate(across(where(is.character), factor)) %>%
dplyr::mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
mtry(range = c(1, 5)),
trees(range = c(100, 500)),
min_n(range = c(20, 50)),
tree_depth(range = c(-1, 10)),
learn_rate(range = c(0.1, 1)),
loss_reduction(range = c(0, 10)),
#sample_size(range = c(0.01, 1)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
mtry(range = c(1, 5)),
trees(range = c(100, 500)),
min_n(range = c(20, 50)),
tree_depth(range = c(1, 10)),
learn_rate(range = c(0.1, 1)),
loss_reduction(range = c(0, 10)),
#sample_size(range = c(0.01, 1)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
grid_search_result
# 최종 모델 object를 생성합니다
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "roc_auc",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
last_fitted_model %>% collect_metrics()
