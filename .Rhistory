# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "klaR"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::naiveBayes_phi(engine = engine,
mode = mode)
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
smoothness(range = c(1.0, 2)),
Laplace(range = c(0.0, 0.5)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
model
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
model
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
install.packages("lightgbm")
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "lightgbm"
mode = "classification"
library(goophi)
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
?parsnip::boost_tree
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
devtools::document()
library(goophi)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
model
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
?boost_tree
install.packages("lightgbm")
library(goophi)
# 함수 추가 시 roxygen 주석을 포함시켜 작성하고, 아래 코드로 주석을 .Rd 파일로 전환 및 NAMESPACE에 추가
devtools::document()
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
?boost_tree
library(treesnip)
library(goophi)
if (! ("devtools" %in% rownames(installed.packages()))) { install.packages("devtools") }
base::require("devtools")
if (! ("roxygen2" %in% rownames(installed.packages()))) { install.packages("roxygen2") }
base::require("roxygen2")
if (! ("testthat" %in% rownames(installed.packages()))) { install.packages("testthat") }
base::require("testthat")
if (! ("knitr" %in% rownames(installed.packages()))) { install.packages("knitr") }
base::require("knitr")
devtools::document()
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
engine = "lightgbm"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::lightGbm_phi(engine = engine,
mode = mode)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(goophi)
set.seed(1234)
## data import
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-c(PassengerId, Name, Cabin, Ticket)) %>%
mutate(across(where(is.character), factor)) %>%
mutate(Survived = as.factor(Survived ))
## one-hot encoding
rec <- recipe(Survived ~ ., data = cleaned_data) %>%
step_dummy(all_predictors(), -all_numeric())
rec_prep <- prep(rec)
cleaned_data <- bake(rec_prep, new_data = cleaned_data)
## 여기까지 완료된 데이터가 전달된다고 가정 (one-hot encoding까지 되는지 확인 필요) ##
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "Survived"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "xgboost"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::xgboost_phi(mtry = tune(),
trees = tune(),
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune(),
sample_size = tune(),
engine = engine,
mode = mode)
model
# 사용자정의 ML 모델을 생성합니다
model <- goophi::xgboost_phi(engine = engine,
mode = mode)
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
min_n(range = c(1, 5)),
mtry(range = c(1, 5)),
trees(range = c(15, 20)),
tree_depth(range = c(6, 10)),
learn_rate(range = c(0, 1)),
loss_reduction(range = c(0, 1)),
sample_size(range = c(1, 1)),
levels = 5)
# trining data를 몇 개로 나눌지 입력받습니다.
v <- 2
parameterGrid
