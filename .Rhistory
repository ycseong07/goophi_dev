formula <- "Survived ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "ranger"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::randomForest_phi(engine = engine,
mode = mode)
model
#### (4) Grid serach CV ####
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
min_n(range = c(10, 40)),
mtry(range = c(1, 5)),
trees(range = c(500, 2000)),
levels = 5)
# training data를 몇 개로 나눌지 입력받습니다.
v <- "2"
parameterGrid
# parameter grid를 적용한 cross validation을 수행합니다
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
grid_search_result
#### (5) Finalize model ####
# 최종 모델 object를 생성합니다
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "roc_auc",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
#### results ####
# performance of final model
last_fitted_model %>% collect_metrics()
last_fitted_model
rec_prep
cleaned_data
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
rec[1]
rec[0]
rec[2]
rec[3]
rec[4]
rec[6]
rec[5]
rec
engine = "ranger"
mode = "classification"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::randomForest_phi(engine = engine,
mode = mode)
model
model[1]
model[2]
model[3]
model[4]
model[0]
model[1]
model
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
min_n(range = c(10, 40)),
mtry(range = c(1, 5)),
trees(range = c(500, 2000)),
levels = 5)
# training data를 몇 개로 나눌지 입력받습니다.
v <- "2"
parameterGrid
# performance of final model
last_fitted_model %>% collect_metrics()
last_fitted_model
last_fitted_model[1]
last_fitted_model[2]
last_fitted_model[[2]]
last_fitted_model[[3]]
last_fitted_model[[4]]
last_fitted_model[[5]]
library(tidyverse)
library(tidymodels)
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
library(vip)
library(ggrepel)
library(ggfortify)
library(ggdendro)
library(goophi)
cleaned_data <- read.csv("~/git/goophi_dev/data/winequality-red.csv", sep = ",")
cleaned_data
#### (1) Train-test split ####
# target 변수를 사용자로부터 입력 받습니다
targetVar <- "quality"
# 아래 3 가지 data를 생성합니다.
data_train <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[1]] # train data
data_test <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[2]] # test data
data_split <- goophi::trainTestSplit(data = cleaned_data, target = targetVar)[[3]] # whole data with split information
#### (2) Make recipe for CV ####
# 아래 변수들을 사용자로부터 입력 받습니다
imputation <- TRUE
normalization <- TRUE
pca <- FALSE ## need to fix warning
formula <- "quality ~ ."
pcaThres <- "0.7"
# train data에 대한 전처리 정보가 담긴 recipe를 생성합니다.
rec <- goophi::preprocessing(data = data_train,
formula,
imputationType = "mean",
normalizationType = "range", # min-max normalization as default
pcaThres = pcaThres,
imputation = imputation,
normalization = normalization,
pca = pca)
rec
#### (3) Modeling ####
## todo: make goophi to install dependencies when the engine is not installed
# engine, mode 사용자로부터 입력 받습니다
engine = "ranger"
mode = "regression"
# 사용자정의 ML 모델을 생성합니다
model <- goophi::randomForest_phi(engine = engine,
mode = mode)
model
#### (4) Grid serach CV ####
# 모델에 사용되는 parameter들을 사용해 parameterGrid를 입력받습니다 (사용자로부터 parameter grid를 받는 방법 고민)
parameterGrid <- dials::grid_regular(
min_n(range = c(10, 40)),
mtry(range = c(1, 5)),
trees(range = c(500, 2000)),
levels = 5)
# training data를 몇 개로 나눌지 입력받습니다.
v <- 2
# parameter grid를 적용한 cross validation을 수행합니다
grid_search_result <- goophi::gridSerachCV(rec = rec,
model = model,
v = v,
data = data_train,
parameterGrid = parameterGrid
)
grid_search_result
#### (5) Finalize model ####
# 최종 모델 object를 생성합니다
finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
metric = "rmse",
model = model,
formula = formula,
trainingData = data_train,
splitedData = data_split)
final_model <- finalized[[1]]
last_fitted_model <- finalized[[2]]
final_model
last_fitted_model
## 아래 부분까지 문제가 없다면 함수화를 마무리합니다
last_fitted_model %>% collect_metrics()
last_fitted_model
last_fitted_model[[5]]
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality) %>%
plot(x = last_fitted_model, y = quality)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality) %>%
autoplot(type = "heatmap")
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality) %>%
autoplot(type = "heatmap")
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality) %>%
autoplot(type = "heatmap")
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality, quality) %>%
autoplot(type = "heatmap")
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(quality)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(quality, quality)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(last_fitted_model$quality, quality)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(last_fitted_model$quality, last_fitted_model$quality)
final_model
last_fitted_model
last_fitted_model[[5]]
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(last_fitted_model[[5]]$quality, last_fitted_model$quality)
?yardstick::metric_set
yardstick::metric_set(rmse, rsq, mae)
multi_metric(data_test, truth = quality, estimate = prediction)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(last_fitted_model, data_test$quality)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(rec, data_test$quality)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(last_fitted_model, last_fitted_model$quality)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(last_fitted_model, last_fitted_model[[5]]$quality)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(last_fitted_model, last_fitted_model$quality)
last_fitted_model[[5]]
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(predict(last_fitted_model), last_fitted_model$quality)
custom_metrics <- yardstick::metric_set(yardstick::accuracy,
yardstick::sens,
yardstick::spec,
yardstick::precision,
yardstick::recall,
yardstick::f_meas,
yardstick::kap,
yardstick::mcc
)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
plot(predict(last_fitted_model), last_fitted_model$quality)
predict(last_fitted_model)
predict(last_fitted_model, newdata = data_test.quality)
custom_metrics <- yardstick::metric_set(yardstick::accuracy,
yardstick::sens,
yardstick::spec,
yardstick::precision,
yardstick::recall,
yardstick::f_meas,
yardstick::kap,
yardstick::mcc
)
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = quality,
estimate = .pred_class)
custom_metrics(last_fitted_model %>%
custom_metrics
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = quality)
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = quality,
estimate = .pred_class)
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = quality,
estimate = quality)
custom_metrics
custom_metrics <- yardstick::metric_set(yardstick::accuracy,
yardstick::sens,
yardstick::spec,
yardstick::precision,
yardstick::recall,
yardstick::f_meas,
yardstick::kap,
yardstick::mcc
)
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = quality,
estimate = quality)
?custom_metrics
?custom_metrics
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = quality,
estimate = 1)
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = as.numeruc(quality),
estimate = 1)
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = as.numeric(quality),
estimate = 1)
custom_metrics(last_fitted_model %>%
tune::collect_predictions(),
truth = data_test,
estimate = 1)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality, .pred_class) %>%
autoplot(type = "heatmap")
last_fitted_model[[5]]
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality, .pred) %>%
autoplot(type = "heatmap")
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality, .pred) %>%
autoplot(type = "heatmap")
last_fitted_model[[5]]
final_model
last_fitted_model
last_fitted_model %>% collect_metrics()
last_fitted_model
last_fitted_model[[5]]
last_fitted_model[[4]]
last_fitted_model[[3]]
last_fitted_model[[2]]
last_fitted_model[[1]]
last_fitted_model[[0]]
last_fitted_model[[6]]
last_fitted_model[[7]]
last_fitted_model[[5]]
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality, .pred) %>%
autoplot(type = "heatmap")
# confusion matrix
last_fitted_model %>%
plot(last_fitted_model[[5]]$quality)
# confusion matrix
last_fitted_model %>%
plot(last_fitted_model$quality)
last_fitted_model[[5.]]
last_fitted_model[[5,]]
.
last_fitted_model[[5.]]
# confusion matrix
last_fitted_model %>%
plot(x = last_fitted_model, y = final_model)
# confusion matrix
last_fitted_model %>%
plot(last_fitted_model)
plot(last_fitted_model)
which.min(last_fitted_model$mse)
which.min(last_fitted_model$rmse)
which.min(last_fitted_model$quality)
# confusion matrix
last_fitted_model %>%
plot(.pred)
# confusion matrix
last_fitted_model %>%
plot(numeric(.pred))
# confusion matrix
last_fitted_model %>%
plot(as.numeric(unlist(.pred)))
# confusion matrix
last_fitted_model %>%
plot(as.numeric(unlist(last_fitted_model)))
# confusion matrix
last_fitted_model %>%
plot(as.numeric(unlist(last_fitted_model$.pred)))
# confusion matrix
last_fitted_model %>%
plot(as.numeric(unlist(last_fitted_model[[.pred]])))
# confusion matrix
last_fitted_model %>%
plot(as.numeric(unlist(last_fitted_model[[1]])))
# confusion matrix
last_fitted_model %>%
plot(as.numeric(unlist(last_fitted_model[[2]])))
last_fitted_model[[1]]
last_fitted_model[[3]]
last_fitted_model[[4]]
last_fitted_model[[5]]
# confusion matrix
last_fitted_model %>%
plot(as.numeric(unlist(last_fitted_model[[4]]$.pred)))
# confusion matrix
last_fitted_model %>%
plot(as.numeric(unlist(last_fitted_model[[5]]$.pred)))
# confusion matrix
last_fitted_model %>%
plot(as.numeric((last_fitted_model[[5]]$.pred)))
last_fitted_model[[5]]$.pred
last_fitted_model[[5]]
last_fitted_model[[5]]$quality
as.numeric(last_fitted_model[[5]])
as.numeric(unlist(last_fitted_model))
predict(model)
df <- data.frame(x1=c(3, 4, 4, 5, 5, 6, 7, 8, 11, 12),
x2=c(6, 6, 7, 7, 8, 9, 11, 13, 14, 14),
y=c(22, 24, 24, 25, 25, 27, 29, 31, 32, 36))
#fit multiple linear regression model
model <- lm(y ~ x1 + x2, data=df)
model
# confusion matrix
last_fitted_model %>%
plot(as.numeric((last_fitted_model[[5]]$.pred)))
#plot predicted vs. actual values
plot(x=predict(model), y=df$y,
xlab='Predicted Values',
ylab='Actual Values',
main='Predicted vs. Actual Values')
plot(x=predict(model), y=df$y,
xlab='Predicted Values',
ylab='Actual Values',
main='Predicted vs. Actual Values')
tmp <- plot(x=predict(model), y=df$y,
xlab='Predicted Values',
ylab='Actual Values',
main='Predicted vs. Actual Values')
tmp
model
predict(model)
# confusion matrix
last_fitted_model %>%
plot(as.numeric((last_fitted_model[[5]]$.pred)))
last_fitted_model
predict(last_fitted_model)
last_fitted_model %>% collect_metrics()
# confusion matrix
plot(last_fitted_model, last_fitted_model)
class(last_fitted_model)
type(last_fitted_model)
?tune::last_fit
plot(last_fitted_model)
plot.lm(last_fitted_model)
plot.lm(last_fitted_model)
plot.function(last_fitted_model)
class(last_fitted_model)
typeof(last_fitted_model)
plot(last_fitted_model[[5]])
typeof(last_fitted_model[[5]])
last_fitted_model[[5]]
plot(last_fitted_model[[5]]$.pred, quality)
plot(last_fitted_model[[5]]$.pred, data_test$quality)
last_fitted_model[[5]]
plot(last_fitted_model[[5]]$.pred, last_fitted_model[[5]]$quality)
last_fitted_model[[5]]$.pred
last_fitted_model[[5]]
last_fitted_model[[5]]$quality
str(last_fitted_model[[5]])
str(last_fitted_model[[5]]$ .pred)
unlist(last_fitted_model[[5]]$ .pred)
unlist(last_fitted_model[[5]])
unlist(last_fitted_model[[5]]$.pred)
last_fitted_model[[5]]
unlist(last_fitted_model[[5]][1:])
last_fitted_model[[5]][1:5]
last_fitted_model[[5]][1]
last_fitted_model[[5]][1][1:5]
last_fitted_model[[5]][[1]][1:5]
last_fitted_model[[5]][[1]][1]
last_fitted_model[[5]][[1]]
last_fitted_model[[5]][[1]][3]
plot(last_fitted_model[[5]][[1]][1], last_fitted_model[[5]][[1]][3])
?plot()
library(goophi)
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality, .pred) %>%
autoplot()
# confusion matrix
last_fitted_model %>%
tune::collect_predictions() %>%
yardstick::conf_mat(quality, .pred) %>%
autoplot()
